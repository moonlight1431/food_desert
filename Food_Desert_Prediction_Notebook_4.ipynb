{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonlight1431/food_desert/blob/main/Food_Desert_Prediction_Notebook_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Notebook #4: INTRO TO EXPLORATORY DATA ANALYSIS (EDA) & DATA CLEANING**##\n",
        "---\n",
        "Please make a copy of this notebook and add it the Notebook #4 folder under the Completed Notebooks folder on the Google Drive and rename the notebook as \"FirstName LastName - Food Desert Prediction - Notebook #4\"\n",
        "\n"
      ],
      "metadata": {
        "id": "sgwROY1PuKEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Icebreaker##\n",
        "\n",
        "\n",
        "*   Spring break plans ðŸ˜Š\n",
        "\n"
      ],
      "metadata": {
        "id": "wrM4iMEeuhF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Midpoint Deliverable**\n",
        "\n",
        "**Subteams** (either 3 pairs or 2 trios):\n",
        "- Team #1: Emmy, Rin, Julina\n",
        "- Team #2: Tiffany, Sidney, Abby\n",
        "---\n",
        "Will take place on **Tuesday, April 1st**. This is **mandatory** for all project members. The midpoint stage requires a mini presentation / screen share to be shared with fellow research teams. Each project member should speak at least once during the presentation. Here's a breakdown of what should be covered:\n",
        "\n",
        "- **[FOR PMs] Project Overview:** Introduce the project idea, purpose, and mention the notebooks we've done so far.\n",
        "- **Subteam Demonstrations:** Each subteam should showcase:\n",
        "  - Current data cleaning processes and results.\n",
        "  - Preliminary data analysis findings.\n",
        "  - At least ONE data visualization.\n",
        "  - Although the dataset all subgroups use will be the same, each of the subteam demonstrations must be unique (aka don't all make the exact same visualizations, the dataset is very big with lots of variables)\n",
        "- **Challenges:** Discuss any obstacles faced and how they were addressed or plan to be tackled.\n",
        "  - Team #1 will be responsible for one challenge\n",
        "  - Team #2 will be responsible for one challenge\n",
        "- **Technical Overview:** Detail the tech stack being utilized, including specific languages and libraries.\n",
        "  - Team #1 will be responsible for this part\n",
        "- **Key Data Science Questions:** Outline the primary data science questions the project aims to answer.\n",
        "  - Team #1 will be responsible for one question\n",
        "  - Team #2 will be responsible for one question\n",
        "- **Looking Ahead:** Share the project's future objectives and anticipated milestones.\n",
        "  - Team #2 will be responsible for this part\n",
        "\n",
        "This structured approach ensures a comprehensive update to all research teams, fostering collaboration and shared learning. Your presentation length should be no more than **10 minutes**, followed by **2 minutes of Q&A** from audience.\n",
        "\n",
        "Each group should try and focus on making 1 data visualization!\n",
        "\n",
        "Here is an example from one of the research project teams from last semester: [[FVA] - Midpoint Deliverable](https://docs.google.com/presentation/d/1z2ciGDNI69RUSXlS6_AQepnVL9-51kAfSwYlNrIOYhk/edit?usp=sharing)\n",
        "\n",
        "Our project spec may also be helpful to you: [Project Spec](https://docs.google.com/document/d/1CuWeaaYyZCRypNdd24m0LsQq9Z5EnEyV_0D95KbDNa8/edit?usp=sharing)\n",
        "\n",
        "All subteams must finish their parts by **Saturday, March 29th** so we can look over them and let you guys know if any changes are necessary! Let Tanushri and Soumily know if you have any questions along the way."
      ],
      "metadata": {
        "id": "xRC3HmMP6eSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis (EDA)**\n",
        "\n",
        "This is the link to the dataset we will be working with: [Food Deserts in the U.S.](https://www.kaggle.com/datasets/tcrammond/food-access-and-food-deserts/data)\n",
        "\n",
        "There should be two datasets that should be in the files folder to the left:\n",
        "- food_access_research_atlas.csv\n",
        "  - The primary file for our EDA, with all our food desert data\n",
        "- food_access_variable_lookup.csv\n",
        "  - File containing the full forms of the column variables for your reference. Can also just use this link if easier: [food_access_variable_lookup on Kaggle](https://www.kaggle.com/datasets/tcrammond/food-access-and-food-deserts?resource=download&select=food_access_variable_lookup.csv)\n",
        "\n",
        "We have used df1 to denote the food_access_research_atlas.csv dataframe."
      ],
      "metadata": {
        "id": "dOJU-mVYNjsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import numpy as np # linear algebra\n",
        "import os # accessing directory structure\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns #data visualizations\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "jBsbHk9oQJ3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nv76Pz_W_kSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
        "# food_access_research_atlas.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n",
        "file_path = \"/content/food_access_research_atlas.csv\"\n",
        "df1 = pd.read_csv(file_path, delimiter=',', nrows = nRowsRead)\n",
        "df1.dataframeName = 'food_access_research_atlas.csv'\n",
        "nRow, nCol = df1.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')"
      ],
      "metadata": {
        "id": "7fYVHpawQO8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take a quick look at what the data looks like:**"
      ],
      "metadata": {
        "id": "Zqkt-CA3Vuig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()  # Show first few rows"
      ],
      "metadata": {
        "id": "fl6wftP6TPZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()  # Check column types & missing values"
      ],
      "metadata": {
        "id": "UVeKAxjvVqvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.describe()  # Summary statistics"
      ],
      "metadata": {
        "id": "Qd5lpmjpVIHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the **Python libraries** we have discussed so far to explore the dataset and generate your own data visualizations. Then, in your slideshow explain what some of these visualizations reveal about the data. Look for trends across the data, correlations between certain factors, and more.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here are some examples of visualizations you can make that we have discussed in previous notebooks:\n",
        "- **Scatter plot**\n",
        "- **Bar graph**\n",
        "- **Heat map**\n",
        "- **Violin plot**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Some Notes:\n",
        "- For some visualizations you may have to use **data cleaning** to make sure your data is ready to use-- see the example of making a heatmap below!\n",
        "- Data cleaning involves removing or correcting data that is incorrect, irrelevant, or improperly formatted\n",
        "- If you are confused on how to create any of these, go back and reference the notebook examples!"
      ],
      "metadata": {
        "id": "-eufNU6vV8h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of a **heatmap** for this dataset to get you guys started:\n",
        "\n",
        "Notice that:\n",
        "- We had to remove the \"State\" and \"County\" columns because for the correlation matrix we want to only include columns with numerical data so that we can find the correlation coefficients\n",
        "- We dropped all columns with NaN (aka null values) to avoid messing with our heatmap"
      ],
      "metadata": {
        "id": "Laa72R_Gdq7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap\n",
        "def plotCorrelationMatrix(df, graphWidth):\n",
        "    df = df.dropna(axis='columns')  # Drop columns with NaN (aka null values)\n",
        "    df = df[[col for col in df if df[col].nunique() > 1]]  # Keep columns with more than 1 unique value\n",
        "    df = df.select_dtypes(include=['number'])  # Remove non-numeric columns\n",
        "    df = df.iloc[:, :10]  # Keep only the first 10 columns <-- can change to only show the heatmap for the columns you want\n",
        "    if df.shape[1] < 2:\n",
        "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
        "        return\n",
        "    corr = df.corr()\n",
        "    plt.figure(figsize=(8,6))\n",
        "    corrMat = plt.matshow(corr)\n",
        "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
        "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
        "    plt.gca().xaxis.tick_bottom()\n",
        "    plt.colorbar(corrMat)\n",
        "    plt.title(f'Correlation Matrix', fontsize=15)\n",
        "    plt.show()\n",
        "df_corr = df1.drop(columns=['State', 'County'])  # Keep only numeric columns (aka no state and county)\n",
        "plotCorrelationMatrix(df_corr, 5)"
      ],
      "metadata": {
        "id": "BYjS-U5_XvP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with code for your own visualizations!"
      ],
      "metadata": {
        "id": "o9o_S6hZX0Zx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}